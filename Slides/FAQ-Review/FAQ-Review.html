<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>FAQ/Review</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.7/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# FAQ/Review

---






&lt;style type="text/css"&gt;
.red{ color: red; }
.blue{ color: blue; }
.huge {
  font-size: 200%;
}
.large {
  font-size: 150%;
}
.tiny {
  font-size: 50%;
}
&lt;/style&gt;

---
class: center, middle

# FAQ

---
class: center, middle, inverse

# Do we have to cross-validate to get metrics on the final model?

---
# Do we have to cross-validate to get metrics on the final model?

--

## YES!!!!

--

Remember the cautionary tale of **overfitting**.

Once we select our final **hyperparameters** and final **predictors**, we fit our final model on the **full data**.  (No data should be wasted!)

--

But we want to report the quality of this model in terms of how we think it will perform on **future data**.

To get a **fair** estimate of the metrics, we need to **cross-validate**.

---
class: center, middle, inverse

# Why do we bother fitting a final model?

---
# Why do we bother fitting a final model?


If we are doing **inference:**  

Because we would like to use **all** the data on hand to get our final statistical estimates (e.g., the **coefficients**)

--


If we are doing **prediction:**

Because we want to prepare to predict on **future observations**.

We would like to use **all** the data on hand to **train** the model that we will use in the future

---
class: center, middle, inverse

# Which metric is the best one?

---
# Which metric is the best one?


**Regression:** While there is no "objective" right answer, the **MSE** and the **R-squared** are popular choices.

--

**Classification:**  This depends very much on **context**.

* How bad is it if we predict A, and the truth is B?
* How bad is it if we predict B, and the truth is A?

---
class: center, middle, inverse

# How do I compute individual metrics?

---
class: center, middle

![](https://media0.giphy.com/media/mCClSS6xbi8us/200.gif)

---
# How do I compute individual metrics?

The **only** time it makes sense to report non-cross-validated metrics is if you have **new data** (or a "validation set").

--

Each metric has its own function, e.g. `roc_auc()`.

This function requires:  

1. The dataset
2. The true classes
3. Either the **predicted classes** or the **predicted probabilities of Class 1**

--

(What is Class 1?  The "first" one in the factor, usually alphabetical.)

---
# How do I compute individual metrics on a validation set?




```r
logit_mod &lt;- logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  set_mode("classification")

ins_recipe &lt;- recipe(smoker ~ charges, 
                     data = ins)

logit_wflow &lt;- workflow() %&gt;%
  add_recipe(ins_recipe) %&gt;%
  add_model(logit_mod)

logit_fit &lt;- logit_wflow %&gt;%
  fit(ins_new)
```

---
# How do I compute individual metrics on a validation set?


```r
predict_classes &lt;- predict(logit_fit, ins_new)
predict_probs &lt;- predict(logit_fit, ins_new,
                         type = "prob")

ins_new &lt;- ins_new %&gt;%
  mutate(
    smoker_predicted = predict_classes$.pred_class,
    smoker_prob_no = predict_probs$.pred_no,
    smoker_prob_yes = predict_probs$.pred_yes
  )
```

---


```
## # A tibble: 143 x 4
##    smoker smoker_predicted smoker_prob_no smoker_prob_yes
##    &lt;fct&gt;  &lt;fct&gt;                     &lt;dbl&gt;           &lt;dbl&gt;
##  1 no     no                        0.996         0.00362
##  2 no     no                        0.956         0.0443 
##  3 no     no                        0.997         0.00296
##  4 no     no                        0.876         0.124  
##  5 no     no                        0.896         0.104  
##  6 no     no                        0.997         0.00341
##  7 no     no                        0.923         0.0770 
##  8 no     no                        0.996         0.00404
##  9 no     no                        0.988         0.0118 
## 10 no     no                        0.979         0.0215 
## # ... with 133 more rows
```

---
# How do I compute individual metrics on a validation set?


```r
ins_new %&gt;%
  accuracy(truth = smoker,
           estimate = smoker_predicted)
```

```
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.902
```

```r
ins_new %&gt;%
  roc_auc(truth = smoker, 
          smoker_prob_no)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.977
```

```r
ins_new %&gt;%
  roc_auc(truth = smoker, 
          smoker_prob_yes,
          event_level = "second")
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.977
```

---
class: center, middle, inverse

# What's the deal with ROC anyways?

---
# What's the deal with ROC anyways?

Our model predictions give us the **probability** of each observation belonging to each category.

--

By default, we'll predict that each observation belongs to the category with the highest probability.

(i.e., in this case, the cutoff is 0.5)

---
# What's the deal with ROC anyways?


```r
ins_new &lt;- ins_new %&gt;%
  mutate(
    cutoff_50 = ifelse(smoker_prob_yes &gt; 0.5, "yes", "no"),
    cutoff_50 = factor(cutoff_50)
  )
```

---


```
## # A tibble: 143 x 5
##    smoker smoker_predicted cutoff_50 smoker_prob_no smoker_prob_yes
##    &lt;fct&gt;  &lt;fct&gt;            &lt;fct&gt;              &lt;dbl&gt;           &lt;dbl&gt;
##  1 no     no               no                 0.996         0.00362
##  2 no     no               no                 0.956         0.0443 
##  3 no     no               no                 0.997         0.00296
##  4 no     no               no                 0.876         0.124  
##  5 no     no               no                 0.896         0.104  
##  6 no     no               no                 0.997         0.00341
##  7 no     no               no                 0.923         0.0770 
##  8 no     no               no                 0.996         0.00404
##  9 no     no               no                 0.988         0.0118 
## 10 no     no               no                 0.979         0.0215 
## # ... with 133 more rows
```
---
# What's the deal with ROC anyways?



```r
ins_new %&gt;%
  sensitivity(truth = smoker,
           estimate = smoker_predicted)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 sens    binary         0.948
```

```r
ins_new %&gt;%
  specificity(truth = smoker,
           estimate = smoker_predicted)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 spec    binary         0.714
```

```r
ins_new %&gt;%
  sensitivity(truth = smoker,
           estimate = cutoff_50)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 sens    binary         0.948
```

```r
ins_new %&gt;%
  specificity(truth = smoker,
           estimate = cutoff_50)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 spec    binary         0.714
```

---
# What's the deal with ROC anyways?

But what if we used a different number as our cutoff?

For example, maybe we really want to prioritize **specificity**: that is, we want to make sure that if we say someone is a smoker, they really are.

--


```r
ins_new &lt;- ins_new %&gt;%
  mutate(
    cutoff_75 = ifelse(smoker_prob_yes &gt; 0.75, "yes", "no"),
    cutoff_75 = factor(cutoff_75)
  )
```

---


```
## # A tibble: 143 x 5
##    smoker smoker_predicted cutoff_50 smoker_prob_no smoker_prob_yes
##    &lt;fct&gt;  &lt;fct&gt;            &lt;fct&gt;              &lt;dbl&gt;           &lt;dbl&gt;
##  1 no     no               no                 0.996         0.00362
##  2 no     no               no                 0.956         0.0443 
##  3 no     no               no                 0.997         0.00296
##  4 no     no               no                 0.876         0.124  
##  5 no     no               no                 0.896         0.104  
##  6 no     no               no                 0.997         0.00341
##  7 no     no               no                 0.923         0.0770 
##  8 no     no               no                 0.996         0.00404
##  9 no     no               no                 0.988         0.0118 
## 10 no     no               no                 0.979         0.0215 
## # ... with 133 more rows
```
---
# What's the deal with ROC anyways?



```r
ins_new %&gt;%
  sensitivity(truth = smoker,
           estimate = smoker_predicted)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 sens    binary         0.948
```

```r
ins_new %&gt;%
  specificity(truth = smoker,
           estimate = smoker_predicted)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 spec    binary         0.714
```

```r
ins_new %&gt;%
  sensitivity(truth = smoker,
           estimate = cutoff_75)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 sens    binary         0.991
```

```r
ins_new %&gt;%
  specificity(truth = smoker,
           estimate = cutoff_75)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 spec    binary         0.607
```

---
# What's the deal with ROC anyways?

Now, imagine if we did that for all possible cutoffs between 0 and 1.

For each cutoff, we'd have a **sensitivity** and **specificity** pair.

--

Let's plot those pairs!


```r
ins_new %&gt;%
  roc_curve(truth = smoker, 
          smoker_prob_yes,
          event_level = "second")
```

---
# What's the deal with ROC anyways?

![](FAQ-Review_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---
# What's the deal with ROC anyways?

A good classifie* can achieve **high specificity** *and* **high sensitivity**.

--

A good classifier doesn't **change its answers** too much when the cutoff changes.

--

If your probabilities are close to 0 or 1, that's good.

If your probabilities are close to 0.5, you're just guessing.

---
class: center, middle

![](https://i.pinimg.com/originals/74/8a/45/748a45314279131390065fc44cc57960.gif)

---
class: center, middle, inverse

# How the heck do we know which models to try?

---

class: center, middle

![](https://media2.giphy.com/media/deLPEITdVodVe/200.gif)

---

class: center, middle

![](https://media1.giphy.com/media/VuW0mhX8r4DmBUkZ9w/source.gif)

---
# Some ideas...


* **Plot** your potential predictors against your response variable.  Start by using just the ones that appear to have some association.

--

* **Backwards selection:** Begin with all your predictors in the model, and drop one at a time to see if the model improves.

--

* **Forwards selection:** Begin with just one predictor in the model, and add in one at a time to see if the model improves. (*I like this one, personally*)

--

* **Every subset selection:** Try every possible combination of predictors that exists.  Yikes.

---

# Regarding Pre-Processing

You *can* just guess-and-check to see which transformations help...

--

... but ideally, you'd choose your pre-processing for a reason.

--

In KNN, we usually **normalize** everything, so that the predictors are on the same **scale**.

--

In regression, we often do **log transformations** or **square root transformation** of data that is **skewed**, to match the **model assumptions**.

--

Maybe you have some **domain knowledge** of the data, that leads you to a certain choice of pre-processing.


---

class: center, middle, inverse

# From now on...

![](https://i.pinimg.com/originals/5a/9a/08/5a9a08fe4d4fccc070170f2238dfbe37.gif)

---
# From now on:

* If I say **fit a classification model**, it is up to you to decide which model types (KNN, Logistic, etc) to try.

--

* If I say **choose the best model**, it is up to you to decide which metric you are using and justify it.

--

* If I do not explicitly say to **consider variable transformations**, you should still consider variable transformations.

--

* If I say **report your final model**, you should also report relevant, cross-validated metrics.


---
class: center, middle, inverse

# Final Advice:

![](https://media1.tenor.com/images/6f017ea3e505c0de7e84a8505972ec5d/tenor.gif?itemid=9502336)

(skip the math!)

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
